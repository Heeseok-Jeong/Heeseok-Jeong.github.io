<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta http-equiv="X-UA-Compatible" content="ie=edge">
  <!--Favicon-->
  <link rel="shortcut icon" href="/assets/favicon.ico" type="image/x-icon">

  <!-- Font Awesome -->
  <link rel="stylesheet" href="https://use.fontawesome.com/releases/v5.9.0/css/all.css"
    integrity="sha384-i1LQnF23gykqWXg6jxC2ZbCbUMxyw5gLZY6UiUS98LYV5unm8GWmfkIS6jqJfb4E" crossorigin="anonymous">

  <!-- Spoqa Han Sans -->
  <link href='//spoqa.github.io/spoqa-han-sans/css/SpoqaHanSans-kr.css' rel='stylesheet' type='text/css'>

  <!-- CSS -->
  <link rel="stylesheet" href="/assets/css/main.css">

  <!-- OG Tag -->
  
  <meta name="title" content="Heeseok Jeong-Ustage Day 17" />
  <meta name="author" content="Heeseok Jeong" />
  <meta name="keywords" content="BoostCamp AI Tech" />
  <meta name="description" content="RNN|LSTM|GRU" />
  <meta name="robots" content="index,follow" />

  <meta property="og:title" content="Heeseok Jeong-Ustage Day 17" />
  <meta property="og:description" content="RNN|LSTM|GRU" />
  <meta property="og:type" content="website, blog" />
  <meta property="og:image"
    content="http://localhost:4000/assets/img/smile.png" />
  <meta property="og:site_name" content="Heeseok Jeong" />
  <meta property="og:url" content="http://localhost:4000/2021/02/16/bcaitech-ustage-day17.html" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Heeseok Jeong-Ustage Day 17" />
  <meta name="twitter:description" content="RNN|LSTM|GRU" />
  <meta name="twitter:image"
    content="http://localhost:4000/assets/img/smile.png" />

  <title>Heeseok Jeong-Ustage Day 17</title>
</head>

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        TeX: {
          equationNumbers: {
            autoNumber: "AMS"
          }
        },
        tex2jax: {
        inlineMath: [ ['$', '$'] ],
        displayMath: [ ['$$', '$$'] ],
        processEscapes: true,
      }
    });
    MathJax.Hub.Register.MessageHook("Math Processing Error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    MathJax.Hub.Register.MessageHook("TeX Jax - parse error",function (message) {
          alert("Math Processing Error: "+message[1]);
        });
    </script>
    <script type="text/javascript" async
      src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
    </script>


<body>
  <div class="container">
    

<header>
  <nav>
    <ul>
      
      <!-- others -->
      <a href="http://localhost:4000">
        <li class="current btn-nav">Blog</li>
      </a>
      <a href="http://localhost:4000/tags">
        <li class="btn-nav">Tags</li>
      </a>
      <a href="http://localhost:4000/portfolio">
        <li class="btn-nav">Portfolio</li>
      </a>
      
    </ul>
  </nav>
</header>
<div id="post">
  <section class="post-header">
    <h1 class="title">Ustage Day 17</h1>
    <p class="subtitle">RNN|LSTM|GRU</p>
    <p class="meta">
      February 16, 2021
    </p>
  </section>
  <section class="post-content">
    <h1 id="목차">목차</h1>

<p><br /></p>

<ul>
  <li><a href="#recurrent-neural-network-and-language-modeling">Recurrent Neural Network and Language Modeling</a></li>
  <li><a href="#lstm-and-gru">LSTM and GRU</a></li>
  <li><a href="#basic-rnn-실습">Basic RNN 실습</a></li>
  <li><a href="#lstm--gru-실습">LSTM, GRU 실습</a></li>
  <li><a href="#preprocessing-for-nmt-model">Preprocessing for NMT Model</a></li>
  <li><a href="#피어-세션">피어 세션</a></li>
  <li><a href="#today-i-felt">Today I Felt</a></li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="recurrent-neural-network-and-language-modeling">Recurrent Neural Network and Language Modeling</h1>

<p><br /></p>

<h2 id="참고">참고</h2>

<p>들어가기 앞서 지난 번에 배운 RNN 첫걸음 내용도 참고하자.</p>
<blockquote>
  <p><a href="https://heeseok-jeong.github.io/2021/02/04/bcaitech-ustage-day14.html">Ustage_day14</a></p>
</blockquote>

<p><br /></p>

<h2 id="basic-of-rnns">Basic of RNNs</h2>

<ul>
  <li>시퀀스 입력 데이터에 대해 현재 입력 $x_t$ 는 이전 입력값들의 계산 결과 $h_\mathit{t-1}$ 와 함께 계산되어 $h_t$ 를 뽑아낸다.</li>
  <li>출력값 y 를 만들기 위해서는 h 에 대해 선형 변환 ($W_\mathit{hy}h$) 수행, 매 스텝마다 뽑아내야할 수도 있고 아닐 수도 있음 (번역 or 요약 등)
    <ul>
      <li>y 는 스칼라 값을 지닌 벡터이므로 softmax 를 통해 분류 등 수행</li>
    </ul>
  </li>
  <li>RNN 함수 $f_W$ 과 $W_y$ 는 모든 스텝에서 동일</li>
  <li>
    <p>h 의 차원 수는 사전에 정의해야 하는 하이퍼파라미터</p>

    <p><img src="/assets/img/ustage_day17/1.png" alt="image1" /></p>
  </li>
  <li>$f_W(h_\mathit{t-1},\ x_t)$ 는 $h_\mathit{t-1},\ x_t$ 를 concat 하여 계산한다는 뜻</li>
</ul>

<h4 id="rnn-종류">RNN 종류</h4>

<p><img src="/assets/img/ustage_day17/2.png" alt="image2" /></p>

<ul>
  <li>one to one(RNN x) : 입력 1개, 출력 1개 (시퀀스 x) (점수 예측)</li>
  <li>one to many : 하나의 입력 (첫 스텝 제외, 나머지 스텝에서는 비어있는 입력 넣음) 으로 여러 스텝을 하며 항상 출력</li>
  <li>many to one : 시퀀스 입력을 스텝마다 처리하여 마지막에 결과 출력 (문장 감정 분석)</li>
  <li>many to many
    <ul>
      <li>시퀀스 입력과 여러 출력 (문장 번역)</li>
      <li>입력, 출력 1대1 대응 (단어 품사 분석, 영상 프레임마다 어떤 장면인지 분석 등)</li>
    </ul>
  </li>
</ul>

<h3 id="character-level-language-model">Character-level Language Model</h3>

<ul>
  <li>“hello” 단어에서 각 문자에 대해 다음 문자 예측 수행</li>
  <li>vocab : [h, e, l, o]</li>
  <li>[1, 0, 0, 0], …, [0, 0, 0, 1]</li>
  <li>h→e, … l→o</li>
  <li>$h_t = tanh(W_\mathit{hh}h_\mathit{t-1}+W_\mathit{xh}x_t+b)$</li>
  <li>many to many</li>
  <li>softmax(y_hat) 와 y 의 차를 loss 로 두어 backpropagation 수행</li>
  <li>
    <p>inference</p>

    <p><img src="/assets/img/ustage_day17/3.png" alt="image3" /></p>
  </li>
</ul>

<h4 id="bptt">BPTT</h4>

<p><img src="/assets/img/ustage_day17/4.png" alt="image4" /></p>

<ul>
  <li>RNN 과정이 많이 반복될 수록 backpropagation 수행 시간 많이 소요</li>
  <li>truncated, 특정 크기만큼만 backpropagetion 수행</li>
</ul>

<h4 id="how-rnn-works">How RNN Works</h4>

<ul>
  <li>
    <p>If statement cell</p>

    <p><img src="/assets/img/ustage_day17/5.png" alt="image5" /></p>

    <ul>
      <li>if 뒤에는 빨간색이 됨, 저 부분을 담당하는 특정 dim 이 학습되었음</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h3 id="vanishingexploding-gradient-problem-in-rnn">Vanishing/Exploding Gradient Problem in RNN</h3>

<ul>
  <li>
    <p>백프로파게이션할 때 같은 매트릭스를 매 스텝마다 곱하면 grad 가 사라지거나 넘침</p>

    <p><img src="/assets/img/ustage_day17/6.png" alt="image6" /></p>

    <ul>
      <li>h3 을 h1 에 대해 편미분해서 내려가다 보면, 3 이 계속 곱해짐. $W_\mathit{hh}$ 를 계속 곱하게 됨.</li>
      <li>학습이 잘 안 됨</li>
    </ul>
  </li>
</ul>

<p><strong>Further Reading</strong></p>

<ul>
  <li><a href="http://karpathy.github.io/2015/05/21/rnn-effectiveness/">The Unreasonable Effectiveness of Recurrent Neural Networks</a></li>
  <li><a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture10.pdf">CS231n(2017)_Lecture10_RNN</a></li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="lstm-and-gru">LSTM and GRU</h1>

<p><br /></p>

<h2 id="lstm-long-short-term-memory">LSTM (Long Short-Term Memory)</h2>

<ul>
  <li>단기 기억을 길게 보관할 수 있도록 만든 소자</li>
  <li>$h_t = f_w(x_t, h_\mathit{t-1})$</li>
  <li>${c_t, h_t} = LSTM(x_t, c_\mathit{t-1}, h_\mathit{t-1})$</li>
  <li>$c_t$ 셀 스테이트
    <ul>
      <li>조금 더 완성된 (여러가지 필요한 정보를 담고 있는) 벡터</li>
    </ul>
  </li>
  <li>$h_t$ 히든 스테이트
    <ul>
      <li>셀 스테이트 벡터를 한 번 더 가공해서 노출시켜주는 벡터</li>
    </ul>

    <p><img src="/assets/img/ustage_day17/7.png" alt="image7" /></p>

    <ul>
      <li>x 와 h 는 h 차원 가져서 concat 하면 2h. 선형결합하면 2h → 4h 가 됨.</li>
      <li>시그모이드를 거치면 값을 줄여줌 (e.g., 원래 값의 30% 만 보존)</li>
      <li>tanh를 거치면 -1~1 사이 값 가지므로 벡터를 저 사이로 변환시켜 유의미한 정보로 만듦</li>
    </ul>
  </li>
  <li>
    <p>Forget gate</p>

    <p><img src="/assets/img/ustage_day17/8.png" alt="image8" /></p>

    <ul>
      <li>시그모이드를 통해 값을 버림</li>
    </ul>
  </li>
  <li>
    <p>Gate gate</p>

    <p><img src="/assets/img/ustage_day17/9.png" alt="image9" /></p>

    <ul>
      <li>C틸다는 tanh 를 거쳐 -1~1 로 바뀜.</li>
      <li>셀 스테이트 갱신 : <strong>덜어낸 정보</strong> (c * forget ) 와 <strong>새로운 정보</strong> ($i_t$ * C틸다 (한 번의 선형변환만으로 더해줄 값을 만들기 어렵기 때문에 C틸다로 값을 만들고 i 를 곱하여 값을 덜어냄)) 를 더한다.</li>
    </ul>
  </li>
  <li>
    <p>Output gate</p>

    <p><img src="/assets/img/ustage_day17/10.png" alt="image10" /></p>

    <ul>
      <li>히든 스테이트 : 만들어진 셀 스테이트에 tanh 를 통해 정보를 만들고 o 만큼 덜어냄.</li>
      <li>히든 스테이트는 많은 정보를 지닌 셀 스테이트를 조정해준 것</li>
      <li>출력의 소스가 됨</li>
    </ul>
  </li>
</ul>

<p><br /></p>

<h2 id="gru-gated-recurrent-unit">GRU (Gated Recurrent Unit)</h2>

<ul>
  <li>경량화되어 적은 메모리 소요 + 속도 빠름</li>
  <li>히든 스테이트 벡터만 존재</li>
  <li>전체 동작 원리는 LSTM 과 거의 비슷함</li>
  <li>
    <p>LSTM 에서 완전한 정보 셀 스테이트처럼 여기서는 히든 스테이트가 완전한 정보를 지녀야 함</p>

    <p><img src="/assets/img/ustage_day17/11.png" alt="image11" /></p>

    <ul>
      <li>z 는 인풋 게이트</li>
      <li>구조를 보면 이전 정보 h 에는 1-z, 현재 인풋 게이트 h틸다에는 z 를 곱하여 덜어낼 것과 새로 채울 것의 비율을 맞춰줌</li>
    </ul>
  </li>
  <li>경량화됐지만 LSTM 과 비슷하거나 좋은 성능 보여줌</li>
</ul>

<h3 id="backpropagation-in-lstmgru">Backpropagation in LSTM?GRU</h3>

<p><img src="/assets/img/ustage_day17/12.png" alt="image12" /></p>

<ul>
  <li>더 길게 grad 를 전달해줄 수 있음</li>
</ul>

<h3 id="요약">요약</h3>

<ul>
  <li>RNN 은 아키텍쳐 디자인에 유연성을 더해줌</li>
  <li>
    <p>기본 RNN 은 간단하지만 잘 동작하지 않음</p>

    <p>→ timestep 마다 업데이트하는 과정이 곱셈에 기반했기 때문에 gradient vanishing 문제 생김</p>
  </li>
  <li>LSTM 과 GRU 는 덧셈 기반으로 grad 잘 전달함</li>
</ul>

<p><strong>Further Reading</strong></p>

<ul>
  <li><a href="http://colah.github.io/posts/2015-08-Understanding-LSTMs/">Understanding LSTM Networks</a></li>
</ul>

<p><strong>Further Question</strong></p>

<ul>
  <li>BPTT 이외에 RNN/LSTM/GRU의 구조를 유지하면서 gradient vanishing/exploding 문제를 완화할 수 있는 방법이 있을까요?<br />
  -&gt; RTRL, EKF (확실하지 않음)</li>
  <li>RNN/LSTM/GRU 기반의 Language Model에서 초반 time step의 정보를 전달하기 어려운 점을 완화할 수 있는 방법이 있을까요?<br />
  -&gt; teacher forcing (초반에는 출력 단어를 입력으로 넣지 않고 답을 입력으로 넣음)</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="basic-rnn-실습">Basic RNN 실습</h1>

<p><br /></p>

<h3 id="필요-패키지-import">필요 패키지 import</h3>

<ul>
  <li>torch, torch.nn, torch.nn.utils.rnn 의 pack_padded_sequence, pad_packed_sequence 임포트</li>
</ul>

<h3 id="데이터-전처리">데이터 전처리</h3>

<ul>
  <li>보캡사이즈 100</li>
  <li>모든 데이터 동일한 크기 가지도록 패딩값 설정 0</li>
  <li>데이터는 문장의 단어들을 인덱스 형태로 지님</li>
  <li>패딩 전처리 (가장 긴 문자열 기준으로 맞춰줌)</li>
  <li>데이터 전체를 배치로 만듦 batch = torch.LongTensor(data) # (B: 배치크기, L: 문장 최대 길이)</li>
</ul>

<h3 id="rnn-사용해보기">RNN 사용해보기</h3>

<ul>
  <li>RNN 에 사용될 word embedding 을 위해 embedding layer 만듦
    <ul>
      <li>embedding = nn.Embedding(vocab_size, embedding_size)</li>
      <li>batch_emb = embedding(batch) # (B, L, d_w)</li>
    </ul>
  </li>
  <li>모델 정의
    <ul>
      <li>RNN 레이어 개수와 hdim, 단방향 or 양방향 세팅</li>
      <li>nn.RNN 제공, 파라미터 세팅해줄것, rnn = nn.RNN(…)</li>
      <li>h_0 라는 레이어 크기는 (레이어개수*방향수, 배치, 히든), 값은 제로로 세팅</li>
    </ul>
  </li>
  <li>모델에 배치 임베딩을 넣어 두 가지 결과 얻음
    <ul>
      <li>hidden_states, h_n = rnn(batch_emb.transpose(0, 1), h_0) # batch_emb 를 L, B, d_h 로 만들기 위해 트랜스포즈 (batch_first = True 주면 원래 모양으로 넣어도 알아서 해줌)
        <ul>
          <li>hidden_states : 각 타임 스텝의 히든 스테이트 벡터</li>
          <li>h_n : 마지막 스텝에서 (앞의 것 포함한) 의미를 압축한 히든 스테이트 벡터</li>
        </ul>
      </li>
    </ul>
  </li>
</ul>

<h3 id="rnn-활용법">RNN 활용법</h3>

<ul>
  <li>위에서 나온 아웃풋들을 사용해보자
    <ul>
      <li>1) 마지막 히든 스테이트를 사용하여 텍스트 분류 문제 풀기 (매니 투 원)
        <ul>
          <li>분류하고자 하는 클래스 개수만큼 리니어 수행</li>
        </ul>
      </li>
      <li>2) 각 스텝의 히든 스테이트를 사용하여 토큰 레벨 문제 풀기 (매니 투 매니)
        <ul>
          <li>원하는 클래스 개수만큼 각 단계마다 리니어 수행</li>
        </ul>
      </li>
    </ul>
  </li>
  <li>랭귀지 모델에서는 조금 다르게 사용해야 함 (아웃풋의 워드를 다시 인풋으로 넣어야 하기 때문). 현재는 결과 문장을 이미 알고 넣어주는 것 (예측해서 넣는게 아니라)</li>
</ul>

<h3 id="packedsequence-사용법">PackedSequence 사용법</h3>

<ul>
  <li>앞에서 데이터에 패딩을 줬기 때문에 0 인 부분이 많음 → 비효율적으로 패딩 부분과 연산해버림</li>
  <li>개선 : 길이순으로 데이터를 내림차순 정렬
    <ul>
      <li>sorted_lens, sorted_idx = batch_lens.sort(descending=True)</li>
      <li>sorted_batch = batch[sorted_idx]</li>
    </ul>

    <p>→ 패딩 전까지만 연산 수행 (PackedSequence 역할)</p>
  </li>
  <li>packed_batch = pack_padded_sequence(emb, sorted_lens) 하면 PackedSequence 객체로 반환 # 패딩은 계산 안해줌, 차원 줄어듦</li>
  <li>outputs, outputs_lens = pad_packed_sequence(packed_outputs) # 원래 차원으로 돌려줌</li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="lstm-gru-실습">LSTM, GRU 실습</h1>

<p><br /></p>

<h3 id="필요-패키지-import-데이터-전처리">필요 패키지 import, 데이터 전처리</h3>

<ul>
  <li>위와 동일</li>
</ul>

<h3 id="lstm-사용">LSTM 사용</h3>

<ul>
  <li>cell state 필요. hidden state 모양과 같음</li>
  <li>임베딩 레이어, lstm = nn.LSTM(…), h_0, c_0 생성</li>
  <li>배치 임베딩</li>
  <li>packed_batch = pack_padded_sequence…</li>
  <li>packed_outputs, (h_n, c_n) = lstm(packed_batch, (h_0, c_0))</li>
  <li>outputs, output_lens = pad_packed_sequence(packed_outputs)</li>
</ul>

<h3 id="gru-사용">GRU 사용</h3>

<ul>
  <li>cell state 없이 hidden state 만 있음</li>
  <li>GRU 를 사용하여 랭귀지 모델 태스크 수행</li>
  <li>이 모델은 teacher forcing 하지 않음 (출력 단어를 입력으로 넣지 않고 답을 입력으로 넣는 방식)</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># gru 생성
</span><span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">GRU</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span> <span class="k">if</span> <span class="n">num_dirs</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">False</span>
<span class="p">)</span>

<span class="c1"># 출력 레이어 생성
</span><span class="n">output_layer</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">,</span> <span class="n">vocab_size</span><span class="p">)</span>

<span class="c1"># 입력에 사용할 첫 단어 추출
</span><span class="n">input_id</span> <span class="o">=</span> <span class="n">batch</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:]</span>  <span class="c1"># (B)
</span><span class="n">hidden</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_dirs</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_size</span><span class="p">))</span>  <span class="c1"># (1, B, d_h)
</span>
<span class="c1"># 학습
</span><span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">max_len</span><span class="p">):</span>
        <span class="c1"># 한 단어이므로 셀 하나만 통과함
</span>    <span class="n">input_emb</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">input_id</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (1, B, d_w)
</span>    <span class="n">output</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">gru</span><span class="p">(</span><span class="n">input_emb</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>  <span class="c1"># output: (1, B, d_h), hidden: (1, B, d_h)
</span>
        <span class="c1"># 다음 단어를 예측함
</span>    <span class="c1"># V: vocab size
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">output_layer</span><span class="p">(</span><span class="n">output</span><span class="p">)</span>  <span class="c1"># (1, B, V)
</span>    <span class="n">probs</span><span class="p">,</span> <span class="n">top_id</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nb">max</span><span class="p">(</span><span class="n">output</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># probs: (1, B), top_id: (1, B)
</span>
    <span class="k">print</span><span class="p">(</span><span class="s">"*"</span> <span class="o">*</span> <span class="mi">50</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="s">f"Time step: </span><span class="si">{</span><span class="n">t</span><span class="si">}</span><span class="s">"</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">output</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">probs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
    <span class="k">print</span><span class="p">(</span><span class="n">top_id</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># 예측 단어를 다음 입력으로 사용
</span>    <span class="n">input_id</span> <span class="o">=</span> <span class="n">top_id</span><span class="p">.</span><span class="n">squeeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># (B)
</span></code></pre></div></div>

<ul>
  <li>현재는 max_len 만큼 문장을 돌지만, seq-seq 에서는 eos 오면 문장 끝냄.</li>
</ul>

<h3 id="양방향-및-여러-layer-사용">양방향 및 여러 layer 사용</h3>

<ul>
  <li>양방향 : 역방향까지 이해를 하면, 순방향 때 파악하지 못했던 것들을 캐치해서 표현력이 좋아짐</li>
  <li>양방향을 concat 해서 사용</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">num_layers</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">num_dirs</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span>

<span class="n">gru</span> <span class="o">=</span> <span class="n">nn</span><span class="p">.</span><span class="n">GRU</span><span class="p">(</span>
    <span class="n">input_size</span><span class="o">=</span><span class="n">embedding_size</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
    <span class="n">num_layers</span><span class="o">=</span><span class="n">num_layers</span><span class="p">,</span>
    <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="o">=</span><span class="bp">True</span> <span class="k">if</span> <span class="n">num_dirs</span> <span class="o">&gt;</span> <span class="mi">1</span> <span class="k">else</span> <span class="bp">False</span>
<span class="p">)</span>

<span class="c1"># d_w: word embedding size, num_layers: layer의 개수, num_dirs: 방향의 개수
</span><span class="n">batch_emb</span> <span class="o">=</span> <span class="n">embedding</span><span class="p">(</span><span class="n">batch</span><span class="p">)</span>  <span class="c1"># (B, L, d_w)
</span><span class="n">h_0</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">num_layers</span> <span class="o">*</span> <span class="n">num_dirs</span><span class="p">,</span> <span class="n">batch</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">hidden_size</span><span class="p">))</span>  <span class="c1"># (num_layers * num_dirs, B, d_h) = (4, B, d_h)
</span>
<span class="n">packed_batch</span> <span class="o">=</span> <span class="n">pack_padded_sequence</span><span class="p">(</span><span class="n">batch_emb</span><span class="p">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span> <span class="n">batch_lens</span><span class="p">)</span>

<span class="n">packed_outputs</span><span class="p">,</span> <span class="n">h_n</span> <span class="o">=</span> <span class="n">gru</span><span class="p">(</span><span class="n">packed_batch</span><span class="p">,</span> <span class="n">h_0</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">packed_outputs</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">packed_outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">shape</span><span class="p">)</span>
<span class="k">print</span><span class="p">(</span><span class="n">h_n</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>

<span class="n">outputs</span><span class="p">,</span> <span class="n">output_lens</span> <span class="o">=</span> <span class="n">pad_packed_sequence</span><span class="p">(</span><span class="n">packed_outputs</span><span class="p">)</span>

<span class="k">print</span><span class="p">(</span><span class="n">outputs</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>  <span class="c1"># (L, B, num_dirs*d_h)
</span><span class="k">print</span><span class="p">(</span><span class="n">output_lens</span><span class="p">)</span>

<span class="c1"># outputs: (max_len, batch_size, num_dir * hidden_size)
# h_n: (num_layers*num_dirs, batch_size, hidden_size)
</span></code></pre></div></div>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="preprocessing-for-nmt-model">Preprocessing for NMT Model</h1>

<p><br /></p>

<h2 id="전처리된-src-trg-문장-반환">전처리된 src, trg 문장 반환</h2>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">...</span>

<span class="c1">### 아래에 코드 빈칸(None)을 완성해주세요
</span>    <span class="n">src_sentence</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_sentence</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="c1"># 기본형 
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">raw_src_sentence</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">src_word2idx</span><span class="p">:</span> <span class="c1"># src dictionary에 현재의 word가 있는 경우
</span>            <span class="n">src_sentence</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">src_word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">src_sentence</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">UNK</span><span class="p">)</span> <span class="c1"># src dictionary에 현재의 word가 없는 경우
</span>    
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">raw_tgt_sentence</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tgt_word2idx</span><span class="p">:</span> <span class="c1"># tgt dictionary에 현재의 word가 있는 경우
</span>            <span class="n">tgt_sentence</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tgt_word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">tgt_sentence</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">UNK</span><span class="p">)</span> <span class="c1"># tgt dictionary에 현재의 word가 없는 경우
</span>
    <span class="c1"># [선택] try, except을 활용해서 조금 더 빠르게 동작하는 코드를 작성해보세요.
</span>    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">raw_src_sentence</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">src_sentence</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">src_word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">])</span> <span class="c1"># src dictionary에 현재의 word가 있는 경우
</span>        <span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
            <span class="n">src_sentence</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">UNK</span><span class="p">)</span> <span class="c1"># src dictionary에 현재의 word가 없는 경우
</span>    
    <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">raw_tgt_sentence</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">tgt_sentence</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">tgt_word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">])</span> <span class="c1"># tgt dictionary에 현재의 word가 있는 경우
</span>        <span class="k">except</span> <span class="nb">KeyError</span><span class="p">:</span>
            <span class="n">tgt_sentence</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">UNK</span><span class="p">)</span> <span class="c1"># tgt dictionary에 현재의 word가 없는 경우
</span>
    <span class="c1"># [선택] List Comprehension을 활용해서 짧은 코드를 작성해보세요. (~2 lines)
</span>    <span class="n">src_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="n">src_word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">src_word2idx</span> <span class="k">else</span> <span class="n">UNK</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">raw_src_sentence</span><span class="p">]</span>
    <span class="n">tgt_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="n">tgt_word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">tgt_word2idx</span> <span class="k">else</span> <span class="n">UNK</span> <span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">raw_tgt_sentence</span><span class="p">]</span>

    <span class="n">src_sentence</span> <span class="o">=</span> <span class="n">src_sentence</span><span class="p">[:</span><span class="n">max_len</span><span class="p">]</span> <span class="c1"># max_len까지의 sequence만
</span>    <span class="n">tgt_sentence</span> <span class="o">=</span> <span class="p">[</span><span class="n">SOS</span><span class="p">]</span> <span class="o">+</span> <span class="n">tgt_sentence</span><span class="p">[:</span><span class="n">max_len</span><span class="o">-</span><span class="mi">2</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">EOS</span><span class="p">]</span> <span class="c1"># SOS, EOS token을 추가하고 max_len까지의 sequence만
</span>
    <span class="c1">### 코드 작성 완료
</span>    <span class="k">return</span> <span class="n">src_sentence</span><span class="p">,</span> <span class="n">tgt_sentence</span>
</code></pre></div></div>

<h2 id="bucketing">Bucketing</h2>

<ul>
  <li>길이가 비슷한 문장들을 묶어서 패딩을 줌 → 시간 효율 높임</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">### 아래에 코드 빈칸(None)을 완성해주세요
</span>    <span class="n">batch_map</span> <span class="o">=</span> <span class="n">defaultdict</span><span class="p">(</span><span class="nb">list</span><span class="p">)</span>
    <span class="n">batch_indices_list</span> <span class="o">=</span> <span class="p">[]</span>
    
    <span class="n">src_len_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">sentence_length</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 첫번째 인덱스인 src의 min length
</span>    <span class="n">tgt_len_min</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">sentence_length</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="nb">len</span><span class="p">)[</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># 두번째 인덱스인 tgt의 min length
</span>
    <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="p">(</span><span class="n">src_len</span><span class="p">,</span> <span class="n">tgt_len</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sentence_length</span><span class="p">):</span>
        <span class="n">src</span> <span class="o">=</span> <span class="p">(</span><span class="n">src_len</span> <span class="o">-</span> <span class="n">src_len_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="n">max_pad_len</span><span class="p">)</span> <span class="c1"># max_pad_len 단위로 묶어주기 위한 몫 (그림에서는 5)
</span>        <span class="n">tgt</span> <span class="o">=</span> <span class="p">(</span><span class="n">tgt_len</span> <span class="o">-</span> <span class="n">tgt_len_min</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="n">max_pad_len</span><span class="p">)</span> <span class="c1"># max_pad_len 단위로 묶어주기 위한 몫 (그림에서는 5)
</span>        <span class="n">batch_map</span><span class="p">[(</span><span class="n">src</span><span class="p">,</span> <span class="n">tgt</span><span class="p">)].</span><span class="n">append</span><span class="p">(</span><span class="n">idx</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">key</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">batch_map</span><span class="p">.</span><span class="n">items</span><span class="p">():</span>
        <span class="n">batch_indices_list</span> <span class="o">+=</span> <span class="p">[</span><span class="n">value</span><span class="p">[</span><span class="n">i</span><span class="p">:</span> <span class="n">i</span><span class="o">+</span><span class="n">batch_size</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">value</span><span class="p">),</span> <span class="n">batch_size</span><span class="p">)]</span>

    <span class="c1">### 코드 작성 완료
</span>
    <span class="c1"># Don't forget shuffling batches because length of each batch could be biased
</span>    <span class="n">random</span><span class="p">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">batch_indices_list</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">batch_indices_list</span>
</code></pre></div></div>

<h2 id="collate-function">Collate Function</h2>

<ul>
  <li>주어진 데이터셋을 원하는 형태의 batch 로 가공하는 함수</li>
  <li>batch 단위별로 max seqence length 에 맞게 pad 추가하고 내림차순 정렬</li>
</ul>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">PAD</span> <span class="o">=</span> <span class="n">Language</span><span class="p">.</span><span class="n">PAD_TOKEN_IDX</span>
    <span class="n">batch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">batched_samples</span><span class="p">)</span>

    <span class="c1">### 아래에 코드 빈칸을 완성해주세요
</span>    <span class="n">batched_samples</span> <span class="o">=</span> <span class="nb">sorted</span><span class="p">(</span><span class="n">batched_samples</span><span class="p">,</span> <span class="n">key</span><span class="o">=</span><span class="k">lambda</span> <span class="n">x</span> <span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">reverse</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># 0번째 요소의 길이를 기준으로 내림차순 정렬
</span>    
    <span class="n">src_sentences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">tgt_sentences</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">src_sentence</span><span class="p">,</span> <span class="n">tgt_sentence</span> <span class="ow">in</span> <span class="n">batched_samples</span><span class="p">:</span>
        <span class="n">src_sentences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">src_sentence</span><span class="p">))</span>
        <span class="n">tgt_sentences</span><span class="p">.</span><span class="n">append</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">tgt_sentence</span><span class="p">))</span>

    <span class="n">src_sentences</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">src_sentences</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># batch x longest seuqence 순으로 정렬 (링크 참고)
</span>    <span class="n">tgt_sentences</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">rnn</span><span class="p">.</span><span class="n">pad_sequence</span><span class="p">(</span><span class="n">tgt_sentences</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span> <span class="c1"># batch x longest seuqence 순으로 정렬 (링크 참고)
</span>    <span class="c1"># 링크: https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pad_sequence.html
</span>
    <span class="c1">### 코드 작성 완료
</span>
    <span class="k">assert</span> <span class="n">src_sentences</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">batch_size</span> <span class="ow">and</span> <span class="n">tgt_sentences</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">==</span> <span class="n">batch_size</span>
    <span class="k">assert</span> <span class="n">src_sentences</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nb">long</span> <span class="ow">and</span> <span class="n">tgt_sentences</span><span class="p">.</span><span class="n">dtype</span> <span class="o">==</span> <span class="n">torch</span><span class="p">.</span><span class="nb">long</span>
    <span class="k">return</span> <span class="n">src_sentences</span><span class="p">,</span> <span class="n">tgt_sentences</span>
</code></pre></div></div>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="피어-세션">피어 세션</h1>

<p><br /></p>

<h2 id="수업-질문">수업 질문</h2>

<ul>
  <li><a href="https://github.com/boostcamp-ai-tech-4/peer-session/issues/65">[히스] Word2Vector 의 목적</a></li>
  <li><a href="https://github.com/boostcamp-ai-tech-4/peer-session/issues/66">[히스] LSTM과 GRU 는 백프로파게이션을 셀 스테이트에 대해서만 하나요?</a></li>
  <li><a href="https://github.com/boostcamp-ai-tech-4/peer-session/issues/67">[펭귄] 양방향 RNN/LSTM은 어떤 식으로 학습을 하나요?</a></li>
</ul>

<p><br /></p>

<hr />

<p><br /></p>

<h1 id="today-i-felt">Today I Felt</h1>

<p><br /></p>

<h2 id="온라인-커뮤니케이션">온라인 커뮤니케이션</h2>

<p>나름 커뮤니케이션을 잘한다고 생각하며 살아왔는데, 온라인에서는 서로 서로 말할 타이밍을 잡아야하고, 얼굴로 감정을 보지 않고 소리가 주가 되다보니 오프라인 커뮤니케이션보다 힘든 점이 있다. 앞으로 온라인 커뮤니케이션이 더 활발해질지도 모르는 세상이기에 조금 더 온라인 커뮤니케이션에 맞는 역량을 키울 필요성을 느꼈다.</p>

<p><br /></p>

<p>어제 클럽하우스에서 성킴님을 비롯한 여러 자연어 처리 전문가들이 GPT-3 에 대해 토론을 했다. 토론 내용도 너무 좋았지만, 온라인 커뮤니케이션 관점에서 성킴님의 모더레이터 역할에서 배울 점을 발견할 수 있었다. 본인이 말하기보다는 적절히 다른 사람들이 말할 기회를 열어주고, 전체 흐름 환기, 조율 등이 그것이다.<br />
앞으로 피어 세션을 기회 삼아 조금 더 온라인 커뮤니케이션 능력을 길러야겠다 :)</p>

  </section>
</div>

<div id="top" class="top-btn" onclick="moveTop()">
  <i class="fas fa-chevron-up"></i>
</div>

<!-- Disqus -->

<div id="comments">
  <div class="border">
    <div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname = 'heeseok-jeong';
  (function () {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript></noscript>
  </div>
</div>


<!-- Footer -->
<footer>
  <div class="footer">
    Copyright © 2019
    <a href="https://github.com/NAYE0NG">Nayeong Kim</a>.
    Powered by Jekyll with
    <a href="https://github.com/naye0ng/Grape-Theme">Grape Theme</a>.
  </div>
</footer>


<script>
  var lastScrollTop = 0;
  window.onscroll = function () {
    var st = document.body.scrollTop || document.documentElement.scrollTop;
    if (st > 250) {
      document.getElementById("top").style.display = "block"
      if (st > lastScrollTop) {
        document.getElementById("top").style.opacity = 0
      } else {
        document.getElementById("top").style.opacity = 1
      }
    } else {
      document.getElementById("top").style.opacity = 0
      if (st > lastScrollTop) {
        document.getElementById("top").style.display = "none"
      }
    }
    lastScrollTop = st <= 0 ? 0 : st;
  }
  function moveTop() {
    document.body.scrollTop = 0
    document.documentElement.scrollTop = 0
  }
</script>
  </div>
</body>

</html>